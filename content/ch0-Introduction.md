# 数字人问答系统

## 项目目的

本项目主要目的是带领学员实践一个大语言模型（LLM）项目，我们会从头到尾以业界实际项目开发流程为标准，从一线架构和工程的角度为学员展开讲解。学员不仅能够从零到一构建一个LLM项目，同时更重要的是完成思维方式的转变，以更快适应企业实际工作节奏。

学员应该具备NLP相关算法知识和Python编程（或项目）经历，在学习完教程后应该具备直接对接企业业务需求，并独立完成相关功能设计、开发、上线和迭代全流程。

## 项目介绍

本项目主要是做一个数字人问答系统，叫什么名字不重要。简单来说，就是要做一个针对某个领域、某个人、某本书……等等的，可控制的QA问答系统。

实际中，我们很多项目都是从需求开始，而需求往往是由需求部门发起。所以，清楚对方的需求非常重要，再往深了说，要明白对方要解决什么问题。只有找到真正的问题，才能提出合理的解决方案。

我们现在假设业务方要做一个关于公司各种人力资源政策相关的问答系统。用户是企业内部的员工，需求是根据员工提出的问题返回相应的答复，但是要求这个答复必须基于公司的某个政策文件。

## 技术调研

你可能很想马上开始动手，但是别急。我们先梳理一下大概方案，作为一个工程师，可用、稳定、可控是非常重要的。你可能已经想到了用召回+阅读理解的方式，或者现在是LLM横行的时代，你想用召回+LLM来完成。这是两阶段方案，但其中还有很多细节需要我们确认。

- 召回：
    - 用什么方式召回，关键词能不能解决问题？
    - 是否需要训练模型？
    - 当前最好的SOTA指标如何？
    - 测试数据集要怎么构建？
    - 如果要训练模型，怎么设计损失函数？
    - 怎么搞数据，搞多少数据？
    - 谁来标注数据，怎么培训标注，有没有标注平台？
    - 怎么采样，要不要加各种策略？
- 回答：
    - 当前SOTA指标如何？
    - 数据集构建、评测。
    - LLM能否使用API？是否有数据安全问题？
    - 开源LLM能否商用？
    - 能商用的LLM哪个效果好？而且是在类似任务上效果好？
    - LLM的微调能否做？微调有没有效果？
    - LLM性能咋样，最大并发多少？响应时长怎样？

上面只是列了一部分问题，实际中考虑的可能比这还多，但也可能很多都不用考虑，因为你有Leader；）

如果你的项目经验不太丰富，又是独立负责某项任务，我们强烈建议你首选业界已经很成熟的方案，先保证保质保量完成任务。

## 流程方案

我们这次主要是实践LLM项目，所以毋庸置疑LLM就是我们的选择。但请大家还是不要忘记上面这一步，并完成相关的调研。下面，我们直接进入方案设计。

首先，明确是两阶段，第一步做相关doc召回，第二步，将召回的doc作为上下文和用户的query一起提供给LLM，让它输出回复。一个基本的工作流程如下：

- 收集用户query数据，构建query数据集，可用于微调和测试。
- 拿到所有的人力资源政策文件，并将其转为文本格式。这一步最怕的就是PPT、临时的图片、PDF里的表格等内容；如果自动化的方法无法处理这些内容，而它们又是至关重要的。那要么借助强大的ChatGPT，要么人工标注。
- 对文件切块。我们不太可能将一整篇文档索引进数据库，大部分时候都需要将其拆分为一个个子单元。这里有很多种拆分方法，比如简单的按回车，或者按不同级别Header。也可以借助ChatGPT，或者人工标注。
- 将文档索引到数据库，可以同时用关键词和语义索引，也可以只用一种。如果是语义索引，必要的时候需要重新训练模型。
- 占位一个排序模块，用于对召回结果进行排序。我们可以直接选取TopN作为上下文，但如果Top1的精度很高，则上下文就可以缩小N倍，对应性能也会有不少提升（可不一定是N倍提升，大概率不是）。
- 用LLM或微调后的LLM做文档问答。

以上，我们未涉及与服务和部署相关的任何内容。同时，在本次教程中，由于篇幅和时间因素，我们会忽略第1-3和第5步，换句话说，重点介绍第4和第6个环节。但学员应当注意，其他环节亦至关重要。

对于这两个环节，我们也有一些约定（嗯，省掉了调研步骤）。

- 召回：选用最新的向量模型[moka-ai/m3e-base · Hugging Face](https://huggingface.co/moka-ai/m3e-base)
- LLM：选用链家扩充词表后的LLaMA：[BelleGroup/BELLE-LLaMA-EXT-7B · Hugging Face](https://huggingface.co/BelleGroup/BELLE-LLaMA-EXT-7B)

召回我们也不会过多关注，不过我们会重点介绍LLM相关的大部分算法基础知识，以及LLM相关的微调。

## 内容概览

如上所述，我们会把重心放在LLM上面，因此会围绕LLM进行内容设计。学习路线大致如下：

- 整体介绍：本节内容。
- 文档表示：主要介绍常见和最新的文本表示方式，虽然我们不做这方面的训练，但会介绍与训练相关的知识。
- GPT相关算法：主要介绍GPT相关的重要概念，如Attention、Mask、In-Context Learning、Instruct Tuning等，我们会从统一的视角重新审视这些概念。
- 大模型架构：主要介绍常见的大模型架构和它们的异同，另外也会重点介绍几个LLM时代比较流行的架构，如LLaMA、RWKV、ChatGLM等。
- 全量微调：主要介绍全量微调模型相关知识，尤其是词表扩充，这对于像LLaMA这样的英语为主的模型来说是很有必要的。考虑到资源情况，我们不要求学员必须实操。
- 高效微调：主要介绍常见高效微调方法原理，从整体上对其进行理解。我们会挑选1-2种高效微调进行相应实战，这部分内容是必须要掌握的，从原理到实践。
- LLM生成：主要介绍生成相关的Decoding策略，以及如何从算法角度控制文本生成。这个在实际中会比较重要，很多时候参数的微小调整可能带来效果的巨大变化。
- 模型部署：主要介绍模型部署的基础知识和概念，以及实践如何用Triton完成模型部署。这里，我们不仅会部署LLM模型，同时也会部署向量召回模型。
- 项目开发：主要介绍项目工程开发相关知识，这部分内容不涉及算法，但会涉及测试驱动、设计模式、API设计等编程知识。我们不光要写出正确的代码，而且要整洁、高效、易扩展。